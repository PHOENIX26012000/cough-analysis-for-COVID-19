{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data from Edge Impulse. You can obtain the URL from the Dashboard, right-click on the download icon next to 'Spectral features data' and 'Spectral features labels', and click **Copy link location**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "API_KEY = 'ei_a4a199d2c331dd57709baf622fca6b83528f34504943bad629b613d38a1e4b17'\n",
    "\n",
    "X = (requests.get('https://studio.edgeimpulse.com/v1/api/3973/training/7/x', headers={'x-api-key': API_KEY})).content\n",
    "Y = (requests.get('https://studio.edgeimpulse.com/v1/api/3973/training/7/y', headers={'x-api-key': API_KEY})).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store the data in a temporary file, and load it back through Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('x_train.npy', 'wb') as file:\n",
    "    file.write(X)\n",
    "with open('y_train.npy', 'wb') as file:\n",
    "    file.write(Y)\n",
    "X = np.load('x_train.npy')\n",
    "Y = np.load('y_train.npy')[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our labels and split the data up in a test and training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, random\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Set random seeds for repeatable results\n",
    "RANDOM_SEED = 3\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "classes_values = [ \"cough\", \"no_cough\" ]\n",
    "classes = len(classes_values)\n",
    "\n",
    "Y = tf.keras.utils.to_categorical(Y - 1, classes)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=1)\n",
    "\n",
    "input_length = X_train[0].shape[0]\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
    "\n",
    "def set_batch_size(batch_size, train_dataset, validation_dataset):\n",
    "    shuffle_buffer_size = batch_size * 3\n",
    "    train_dataset = train_dataset.shuffle(shuffle_buffer_size).batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "    validation_dataset = validation_dataset.shuffle(shuffle_buffer_size).batch(batch_size, drop_remainder=True).prefetch(1)\n",
    "    return train_dataset, validation_dataset\n",
    "\n",
    "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "550/550 - 3s - loss: 0.6808 - accuracy: 0.5545 - val_loss: 0.6494 - val_accuracy: 0.5408\n",
      "Epoch 2/200\n",
      "550/550 - 1s - loss: 0.6618 - accuracy: 0.6200 - val_loss: 0.6377 - val_accuracy: 0.6837\n",
      "Epoch 3/200\n",
      "550/550 - 1s - loss: 0.6502 - accuracy: 0.6600 - val_loss: 0.6289 - val_accuracy: 0.6837\n",
      "Epoch 4/200\n",
      "550/550 - 1s - loss: 0.6408 - accuracy: 0.6600 - val_loss: 0.6220 - val_accuracy: 0.6837\n",
      "Epoch 5/200\n",
      "550/550 - 1s - loss: 0.6323 - accuracy: 0.6673 - val_loss: 0.6159 - val_accuracy: 0.6837\n",
      "Epoch 6/200\n",
      "550/550 - 1s - loss: 0.6238 - accuracy: 0.6709 - val_loss: 0.6096 - val_accuracy: 0.6837\n",
      "Epoch 7/200\n",
      "550/550 - 1s - loss: 0.6154 - accuracy: 0.6818 - val_loss: 0.6033 - val_accuracy: 0.7245\n",
      "Epoch 8/200\n",
      "550/550 - 1s - loss: 0.6072 - accuracy: 0.6909 - val_loss: 0.5970 - val_accuracy: 0.7347\n",
      "Epoch 9/200\n",
      "550/550 - 1s - loss: 0.5991 - accuracy: 0.7036 - val_loss: 0.5909 - val_accuracy: 0.7347\n",
      "Epoch 10/200\n",
      "550/550 - 1s - loss: 0.5911 - accuracy: 0.7164 - val_loss: 0.5848 - val_accuracy: 0.7449\n",
      "Epoch 11/200\n",
      "550/550 - 1s - loss: 0.5829 - accuracy: 0.7182 - val_loss: 0.5785 - val_accuracy: 0.7653\n",
      "Epoch 12/200\n",
      "550/550 - 1s - loss: 0.5744 - accuracy: 0.7218 - val_loss: 0.5721 - val_accuracy: 0.7653\n",
      "Epoch 13/200\n",
      "550/550 - 1s - loss: 0.5659 - accuracy: 0.7273 - val_loss: 0.5654 - val_accuracy: 0.7653\n",
      "Epoch 14/200\n",
      "550/550 - 1s - loss: 0.5570 - accuracy: 0.7345 - val_loss: 0.5590 - val_accuracy: 0.7755\n",
      "Epoch 15/200\n",
      "550/550 - 1s - loss: 0.5480 - accuracy: 0.7364 - val_loss: 0.5526 - val_accuracy: 0.7755\n",
      "Epoch 16/200\n",
      "550/550 - 1s - loss: 0.5388 - accuracy: 0.7455 - val_loss: 0.5460 - val_accuracy: 0.7857\n",
      "Epoch 17/200\n",
      "550/550 - 1s - loss: 0.5296 - accuracy: 0.7473 - val_loss: 0.5392 - val_accuracy: 0.7755\n",
      "Epoch 18/200\n",
      "550/550 - 1s - loss: 0.5201 - accuracy: 0.7509 - val_loss: 0.5324 - val_accuracy: 0.7857\n",
      "Epoch 19/200\n",
      "550/550 - 1s - loss: 0.5102 - accuracy: 0.7618 - val_loss: 0.5253 - val_accuracy: 0.7857\n",
      "Epoch 20/200\n",
      "550/550 - 1s - loss: 0.5001 - accuracy: 0.7764 - val_loss: 0.5182 - val_accuracy: 0.7857\n",
      "Epoch 21/200\n",
      "550/550 - 1s - loss: 0.4897 - accuracy: 0.7836 - val_loss: 0.5110 - val_accuracy: 0.7857\n",
      "Epoch 22/200\n",
      "550/550 - 1s - loss: 0.4792 - accuracy: 0.7927 - val_loss: 0.5029 - val_accuracy: 0.7755\n",
      "Epoch 23/200\n",
      "550/550 - 1s - loss: 0.4683 - accuracy: 0.8091 - val_loss: 0.4951 - val_accuracy: 0.8061\n",
      "Epoch 24/200\n",
      "550/550 - 1s - loss: 0.4574 - accuracy: 0.8164 - val_loss: 0.4872 - val_accuracy: 0.8061\n",
      "Epoch 25/200\n",
      "550/550 - 1s - loss: 0.4463 - accuracy: 0.8255 - val_loss: 0.4794 - val_accuracy: 0.8163\n",
      "Epoch 26/200\n",
      "550/550 - 1s - loss: 0.4352 - accuracy: 0.8309 - val_loss: 0.4718 - val_accuracy: 0.8163\n",
      "Epoch 27/200\n",
      "550/550 - 1s - loss: 0.4240 - accuracy: 0.8309 - val_loss: 0.4640 - val_accuracy: 0.8265\n",
      "Epoch 28/200\n",
      "550/550 - 1s - loss: 0.4129 - accuracy: 0.8418 - val_loss: 0.4562 - val_accuracy: 0.8265\n",
      "Epoch 29/200\n",
      "550/550 - 1s - loss: 0.4018 - accuracy: 0.8473 - val_loss: 0.4484 - val_accuracy: 0.8367\n",
      "Epoch 30/200\n",
      "550/550 - 1s - loss: 0.3908 - accuracy: 0.8545 - val_loss: 0.4407 - val_accuracy: 0.8469\n",
      "Epoch 31/200\n",
      "550/550 - 1s - loss: 0.3800 - accuracy: 0.8655 - val_loss: 0.4333 - val_accuracy: 0.8571\n",
      "Epoch 32/200\n",
      "550/550 - 1s - loss: 0.3693 - accuracy: 0.8727 - val_loss: 0.4260 - val_accuracy: 0.8571\n",
      "Epoch 33/200\n",
      "550/550 - 1s - loss: 0.3587 - accuracy: 0.8855 - val_loss: 0.4191 - val_accuracy: 0.8571\n",
      "Epoch 34/200\n",
      "550/550 - 1s - loss: 0.3484 - accuracy: 0.8891 - val_loss: 0.4123 - val_accuracy: 0.8571\n",
      "Epoch 35/200\n",
      "550/550 - 1s - loss: 0.3381 - accuracy: 0.8927 - val_loss: 0.4057 - val_accuracy: 0.8571\n",
      "Epoch 36/200\n",
      "550/550 - 1s - loss: 0.3281 - accuracy: 0.8982 - val_loss: 0.3995 - val_accuracy: 0.8571\n",
      "Epoch 37/200\n",
      "550/550 - 1s - loss: 0.3184 - accuracy: 0.9055 - val_loss: 0.3934 - val_accuracy: 0.8571\n",
      "Epoch 38/200\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, InputLayer, Dropout, Conv1D, Conv2D, Flatten, SeparableConv1D, Reshape, MaxPooling1D, AveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "# model architecture\n",
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(input_length, ), name='x_input'))\n",
    "model.add(Reshape((int(input_length / 13), 13), input_shape=(input_length, )))\n",
    "model.add(Conv1D(30, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, padding='same'))\n",
    "model.add(SeparableConv1D(10, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(classes, activation='softmax', name='y_pred'))\n",
    "\n",
    "# this controls the learning rate\n",
    "opt = Adam(lr=0.00005, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "# this controls the batch size, or you can manipulate the tf.data.Dataset objects yourself\n",
    "BATCH_SIZE = 1\n",
    "train_dataset, validation_dataset = set_batch_size(BATCH_SIZE, train_dataset, validation_dataset)\n",
    "\n",
    "# train the neural network\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "history = model.fit(train_dataset, epochs=200, validation_data=validation_dataset, verbose=2, callbacks=[callbacks])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def visualize_training(history, lw = 3):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(history.history['acc'], label = 'training', marker = '*', linewidth = lw)\n",
    "    plt.plot(history.history['val_acc'], label = 'validation', marker = 'o', linewidth = lw)\n",
    "    plt.title('Training Accuracy vs Validation Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(fontsize = 'x-large')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(history.history['loss'], label = 'training', marker = '*', linewidth = lw)\n",
    "    plt.plot(history.history['val_loss'], label = 'validation', marker = 'o', linewidth = lw)\n",
    "    plt.title('Training Loss vs Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(fontsize = 'x-large')\n",
    "    plt.show()\n",
    "visualize_training(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk\n",
    "model.save('saved_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions and Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    "predictions = model.predict(validation_dataset)\n",
    "predictions[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_indices=np.argmax(predictions,axis=1)\n",
    "predicted_class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
